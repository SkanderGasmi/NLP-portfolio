# NLP-portfolio

Welcome to my NLP portfolio ! 

A central repository and a collection of my **Natural Language Processing (NLP)** experiments, labs and projects. From text classification to sequence modeling, this portfolio showcases my work in the world of NLP.

This central repository serves as a gateway to **four** distinct sub-repositories, each focusing on different aspects of Natural Language Processing (NLP). Whether you're interested in classification, vector spaces, probabilistic models, sequence models, or attention models, you'll find valuable resources and code examples here.

Please explore the individual sub-repositories for specific NLP topics:

-----------------------------------------------------------------------------------------------------------------------------
1. [NLP with Classification and Vector Spaces](https://github.com/SkanderGasmi/NLP-with-classification-and-vector-spaces)
This sub-repository delves into the fundamentals of text classification and vector space models. Whether you're working on sentiment analysis, text categorization, or document retrieval, you'll find resources and code samples to help you get started.

-----------------------------------------------------------------------------------------------------------------------------
2. [NLP with Probabilistic Models](https://github.com/SkanderGasmi/NLP-with-Probabilistic-Models)
In this sub-repository, we explore the world of probabilistic models in NLP. You'll discover techniques like Hidden Markov Models (HMMs), Naive Bayes, and Conditional Random Fields (CRFs) applied to various NLP tasks such as part-of-speech tagging and named entity recognition.

-----------------------------------------------------------------------------------------------------------------------------
3. [NLP with Sequence Models](https://github.com/SkanderGasmi/NLP-with-Sequence-Models)
- Sequence models are crucial in NLP for tasks like machine translation, speech recognition, and text generation. Dive into this sub-repository to find resources and code examples for building and training sequence models such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.

-----------------------------------------------------------------------------------------------------------------------------
7. [NLP with Attention Models](https://github.com/SkanderGasmi/NLP-With-Attention-Models)
Attention models have revolutionized NLP by improving the handling of long sequences and capturing context effectively. In this sub-repository, explore various attention mechanisms, including self-attention and transformer models, and their applications in tasks like language translation and summarization.

-----------------------------------------------------------------------------------------------------------------------------
